{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d17a19c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "        Convolution2D,\n",
    "        LocallyConnected2D,\n",
    "        MaxPooling2D,\n",
    "        Flatten,\n",
    "        Dense,\n",
    "        Dropout,\n",
    "    )\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from imutils import paths\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import subprocess\n",
    "import math\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "\n",
    "import deepface\n",
    "from deepface import DeepFace\n",
    "\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a1d57d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 512\n",
    "IMG_SIZE = 160\n",
    "IMAGE_SIZE = (160, 160)\n",
    "\n",
    "EPOCHS = 5\n",
    "batch_size = 5\n",
    "\n",
    "feature_model = 'Facenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "800a93e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findCosineDistance(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    if isinstance(source_representation, list):\n",
    "        source_representation = np.array(source_representation)\n",
    "\n",
    "    if isinstance(test_representation, list):\n",
    "        test_representation = np.array(test_representation)\n",
    "\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance\n",
    "\n",
    "\n",
    "def l2_normalize(x):\n",
    "    return x / np.sqrt(np.sum(np.multiply(x, x)))\n",
    "\n",
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    img = (img - mean) / std\n",
    "    return img\n",
    "\n",
    "def findThreshold(model_name, distance_metric):\n",
    "\n",
    "    base_threshold = {\"cosine\": 0.40, \"euclidean\": 0.55, \"euclidean_l2\": 0.75}\n",
    "\n",
    "    thresholds = {\n",
    "        \"VGG-Face\": {\"cosine\": 0.40, \"euclidean\": 0.60, \"euclidean_l2\": 0.86},\n",
    "        \"Facenet\": {\"cosine\": 0.40, \"euclidean\": 10, \"euclidean_l2\": 0.80},\n",
    "        \"Facenet512\": {\"cosine\": 0.30, \"euclidean\": 23.56, \"euclidean_l2\": 1.04},\n",
    "        \"ArcFace\": {\"cosine\": 0.68, \"euclidean\": 4.15, \"euclidean_l2\": 1.13},\n",
    "        \"Dlib\": {\"cosine\": 0.07, \"euclidean\": 0.6, \"euclidean_l2\": 0.4},\n",
    "        \"SFace\": {\"cosine\": 0.593, \"euclidean\": 10.734, \"euclidean_l2\": 1.055},\n",
    "        \"OpenFace\": {\"cosine\": 0.10, \"euclidean\": 0.55, \"euclidean_l2\": 0.55},\n",
    "        \"DeepFace\": {\"cosine\": 0.23, \"euclidean\": 64, \"euclidean_l2\": 0.64},\n",
    "        \"DeepID\": {\"cosine\": 0.015, \"euclidean\": 45, \"euclidean_l2\": 0.17},\n",
    "    }\n",
    "\n",
    "    threshold = thresholds.get(model_name, base_threshold).get(distance_metric, 0.4)\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d99896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alignment_procedure(img, left_eye, right_eye):\n",
    "    # this function aligns given face in img based on left and right eye coordinates\n",
    "\n",
    "    left_eye_x, left_eye_y = left_eye\n",
    "    right_eye_x, right_eye_y = right_eye\n",
    "\n",
    "    # -----------------------\n",
    "    # find rotation direction\n",
    "\n",
    "    if left_eye_y > right_eye_y:\n",
    "        point_3rd = (right_eye_x, left_eye_y)\n",
    "        direction = -1  # rotate same direction to clock\n",
    "    else:\n",
    "        point_3rd = (left_eye_x, right_eye_y)\n",
    "        direction = 1  # rotate inverse direction of clock\n",
    "\n",
    "    # -----------------------\n",
    "    # find length of triangle edges\n",
    "\n",
    "    a = findEuclideanDistance(np.array(left_eye), np.array(point_3rd))\n",
    "    b = findEuclideanDistance(np.array(right_eye), np.array(point_3rd))\n",
    "    c = findEuclideanDistance(np.array(right_eye), np.array(left_eye))\n",
    "\n",
    "    # -----------------------\n",
    "\n",
    "    # apply cosine rule\n",
    "\n",
    "    if b != 0 and c != 0:  # this multiplication causes division by zero in cos_a calculation\n",
    "        cos_a = (b * b + c * c - a * a) / (2 * b * c)\n",
    "        angle = np.arccos(cos_a)  # angle in radian\n",
    "        angle = (angle * 180) / math.pi  # radian to degree\n",
    "\n",
    "        # -----------------------\n",
    "        # rotate base image\n",
    "\n",
    "        if direction == -1:\n",
    "            angle = 90 - angle\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        img = np.array(img.rotate(direction * angle))\n",
    "\n",
    "    # -----------------------\n",
    "\n",
    "    return img  # return img anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d4b3e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_detector_model():\n",
    "    import mediapipe as mp  # this is not a must dependency. do not import it in the global level.\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.7)\n",
    "    return face_detection\n",
    "\n",
    "def detect_faces(face_detector, img, align=True):\n",
    "    resp = []\n",
    "\n",
    "    img_width = img.shape[1]\n",
    "    img_height = img.shape[0]\n",
    "\n",
    "    results = face_detector.process(img)\n",
    "\n",
    "    # If no face has been detected, return an empty list\n",
    "    if results.detections is None:\n",
    "        return resp\n",
    "\n",
    "    # Extract the bounding box, the landmarks and the confidence score\n",
    "    for detection in results.detections:\n",
    "        (confidence,) = detection.score\n",
    "\n",
    "        bounding_box = detection.location_data.relative_bounding_box\n",
    "        landmarks = detection.location_data.relative_keypoints\n",
    "\n",
    "        x = int(bounding_box.xmin * img_width)\n",
    "        w = int(bounding_box.width * img_width)\n",
    "        y = int(bounding_box.ymin * img_height)\n",
    "        h = int(bounding_box.height * img_height)\n",
    "\n",
    "        # Extract landmarks\n",
    "        left_eye = (int(landmarks[0].x * img_width), int(landmarks[0].y * img_height))\n",
    "        right_eye = (int(landmarks[1].x * img_width), int(landmarks[1].y * img_height))\n",
    "        # nose = (int(landmarks[2].x * img_width), int(landmarks[2].y * img_height))\n",
    "        # mouth = (int(landmarks[3].x * img_width), int(landmarks[3].y * img_height))\n",
    "        # right_ear = (int(landmarks[4].x * img_width), int(landmarks[4].y * img_height))\n",
    "        # left_ear = (int(landmarks[5].x * img_width), int(landmarks[5].y * img_height))\n",
    "\n",
    "        if x > 0 and y > 0:\n",
    "            detected_face = img[y : y + h, x : x + w]\n",
    "            img_region = [x, y, w, h]\n",
    "\n",
    "            if align:\n",
    "                detected_face = alignment_procedure(detected_face, left_eye, right_eye)\n",
    "\n",
    "            resp.append((detected_face, img_region, confidence))\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dc662e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_faces(\n",
    "    img,\n",
    "    target_size=(160, 160),\n",
    "    detector_backend=\"mediapipe\",\n",
    "    grayscale=False,\n",
    "    enforce_detection=True,\n",
    "    align=True,\n",
    "):\n",
    "    \"\"\"Extract faces from an image.\n",
    "\n",
    "    Args:\n",
    "        img: a path, url, base64 or numpy array.\n",
    "        target_size (tuple, optional): the target size of the extracted faces.\n",
    "        Defaults to (224, 224).\n",
    "        detector_backend (str, optional): the face detector backend. Defaults to \"opencv\".\n",
    "        grayscale (bool, optional): whether to convert the extracted faces to grayscale.\n",
    "        Defaults to False.\n",
    "        enforce_detection (bool, optional): whether to enforce face detection. Defaults to True.\n",
    "        align (bool, optional): whether to align the extracted faces. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if face could not be detected and enforce_detection is True.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of extracted faces.\n",
    "    \"\"\"\n",
    "\n",
    "    # this is going to store a list of img itself (numpy), it region and confidence\n",
    "    extracted_faces = []\n",
    "\n",
    "    # img might be path, base64 or numpy array. Convert it to numpy whatever it is.\n",
    "    #img = load_image(img)\n",
    "    img_region = [0, 0, img.shape[1], img.shape[0]]\n",
    "\n",
    "    #if detector_backend == \"skip\":\n",
    "    #    face_objs = [(img, img_region, 0)]\n",
    "    #else:\n",
    "    #    face_detector = FaceDetector.build_model(detector_backend)\n",
    "    #    face_objs = FaceDetector.detect_faces(face_detector, detector_backend, img, align)\n",
    "    face_detector = build_detector_model()\n",
    "    face_objs = detect_faces(face_detector, img, align)\n",
    "\n",
    "    \n",
    "    \n",
    "    # in case of no face found\n",
    "    if len(face_objs) == 0 and enforce_detection is True:\n",
    "        raise ValueError(\n",
    "            \"Face could not be detected. Please confirm that the picture is a face photo \"\n",
    "            + \"or consider to set enforce_detection param to False.\"\n",
    "        )\n",
    "\n",
    "    if len(face_objs) == 0 and enforce_detection is False:\n",
    "        face_objs = [(img, img_region, 0)]\n",
    "\n",
    "    for current_img, current_region, confidence in face_objs:\n",
    "        if current_img.shape[0] > 0 and current_img.shape[1] > 0:\n",
    "            if grayscale is True:\n",
    "                current_img = cv2.cvtColor(current_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # resize and padding\n",
    "            if current_img.shape[0] > 0 and current_img.shape[1] > 0:\n",
    "                factor_0 = target_size[0] / current_img.shape[0]\n",
    "                factor_1 = target_size[1] / current_img.shape[1]\n",
    "                factor = min(factor_0, factor_1)\n",
    "\n",
    "                dsize = (\n",
    "                    int(current_img.shape[1] * factor),\n",
    "                    int(current_img.shape[0] * factor),\n",
    "                )\n",
    "                current_img = cv2.resize(current_img, dsize)\n",
    "\n",
    "                diff_0 = target_size[0] - current_img.shape[0]\n",
    "                diff_1 = target_size[1] - current_img.shape[1]\n",
    "                if grayscale is False:\n",
    "                    # Put the base image in the middle of the padded image\n",
    "                    current_img = np.pad(\n",
    "                        current_img,\n",
    "                        (\n",
    "                            (diff_0 // 2, diff_0 - diff_0 // 2),\n",
    "                            (diff_1 // 2, diff_1 - diff_1 // 2),\n",
    "                            (0, 0),\n",
    "                        ),\n",
    "                        \"constant\",\n",
    "                    )\n",
    "                else:\n",
    "                    current_img = np.pad(\n",
    "                        current_img,\n",
    "                        (\n",
    "                            (diff_0 // 2, diff_0 - diff_0 // 2),\n",
    "                            (diff_1 // 2, diff_1 - diff_1 // 2),\n",
    "                        ),\n",
    "                        \"constant\",\n",
    "                    )\n",
    "\n",
    "            # double check: if target image is not still the same size with target.\n",
    "            if current_img.shape[0:2] != target_size:\n",
    "                current_img = cv2.resize(current_img, target_size)\n",
    "\n",
    "            # normalizing the image pixels\n",
    "            # what this line doing? must?\n",
    "            img_pixels = image.img_to_array(current_img)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "            img_pixels /= 255  # normalize input in [0, 1]\n",
    "\n",
    "            # int cast is for the exception - object of type 'float32' is not JSON serializable\n",
    "            region_obj = {\n",
    "                \"x\": int(current_region[0]),\n",
    "                \"y\": int(current_region[1]),\n",
    "                \"w\": int(current_region[2]),\n",
    "                \"h\": int(current_region[3]),\n",
    "            }\n",
    "\n",
    "            extracted_face = [img_pixels, region_obj, confidence]\n",
    "            extracted_faces.append(extracted_face)\n",
    "\n",
    "    if len(extracted_faces) == 0 and enforce_detection == True:\n",
    "        raise ValueError(\n",
    "            f\"Detected face shape is {img.shape}. Consider to set enforce_detection arg to False.\"\n",
    "        )\n",
    "\n",
    "    return extracted_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d4b763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give facial image after extraction\n",
    "# face_obj = extract_faces(img, target_size = target_frame_size, enforce_detection = False)\n",
    "\n",
    "def get_embedding(img):\n",
    "    img = np.reshape(img, (1, img.shape[0],img.shape[1],img.shape[2]))\n",
    "    emb = np.array(feature_extractor([img]))\n",
    "    return emb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d91206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_identity(img, embeddings):\n",
    "    target_emb = np.squeeze(get_embedding(img))\n",
    "    prev_dist = 100\n",
    "    identity = ''\n",
    "    for key in embeddings.keys():\n",
    "        dist = findCosineDistance(embeddings[key], target_emb)\n",
    "        if dist < prev_dist:\n",
    "            prev_dist = dist\n",
    "            identity = key\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    if prev_dist > 0.4:\n",
    "        identity = 'Not Found!!'\n",
    "        \n",
    "    return identity, prev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import facenet_model\n",
    "facenet_path = 'models/feature_extractors/facenet512_weights.h5'\n",
    "feature_extractor = facenet_model.Facenet512(dimension = 512, weights_path = facenet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9ea82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_representations(database_path, representations_path):\n",
    "    embeddings = {}\n",
    "    target_frame_size = (IMG_SIZE, IMG_SIZE)\n",
    "    students = os.listdir(database_path)\n",
    "    \n",
    "    for student in students:\n",
    "        dir_path = os.path.join(database_path, student)\n",
    "        student_imgs = os.listdir(dir_path)\n",
    "        img_path = os.path.join(dir_path, student_imgs[0])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = img[:, :, [2, 1, 0]]\n",
    "        target_frame_size = (IMG_SIZE,IMG_SIZE)\n",
    "        face_obj = extract_faces(img, target_size = target_frame_size, enforce_detection = False)\n",
    "        emb  = np.squeeze(get_embedding(face_obj[0][0][0]))\n",
    "        embeddings[student] = emb\n",
    "\n",
    "    # save dictionary \n",
    "    with open(representations_path + '/representations.pkl', 'wb') as fp:\n",
    "        pickle.dump(embeddings, fp)\n",
    "        print('Face embeddings saved successfully to file')\n",
    "        fp.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02a4ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_representations(representations_path):\n",
    "    try:\n",
    "        with open(representations_path + '/representations.pkl', 'rb') as fp:\n",
    "            embeddings = pickle.load(fp)\n",
    "            fp.close()\n",
    "    except:\n",
    "        embeddings = None\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8967292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_representation(identity, new_embedding, representations_path = 'representations'):\n",
    "    embeddings = load_representations(representations_path)\n",
    "    \n",
    "    embeddings[identity] = new_embedding\n",
    "    \n",
    "    # save dictionary \n",
    "    with open(representations_path + '/representations.pkl', 'wb') as fp:\n",
    "        pickle.dump(embeddings, fp)\n",
    "        print('Face embeddings saved successfully to file')\n",
    "        fp.close()\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e882a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face embeddings saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "# For first time entry\n",
    "make_representations('database','representations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1fbceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(video_path, database_path = 'database', representations_path = 'representations'):\n",
    "    attendance_df = student_df.copy()\n",
    "    attendance_df['Attendance'] = ['Absent' for i in range(student_df.shape[0])]\n",
    "    \n",
    "    if video_path:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "    else: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    embeddings = load_representations(representations_path)\n",
    "    \n",
    "    try:\n",
    "      while True:\n",
    "        ret, frame = cap.read()\n",
    "        identity = 'Editing'\n",
    "           \n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        target_frame_size = (IMG_SIZE,IMG_SIZE)\n",
    "        face_obj = extract_faces(frame, target_size = target_frame_size, enforce_detection = False)\n",
    "        bbox = face_obj[0][1]\n",
    "        x = int(bbox['x'])\n",
    "        y = int(bbox['y'])\n",
    "        w = int(bbox['w'])\n",
    "        h = int(bbox['h'])\n",
    "        \n",
    "        identity, distance = find_target_identity(face_obj[0][0][0], embeddings)\n",
    "        if identity == 'Not Found!!':\n",
    "            name = ''\n",
    "            continue\n",
    "        else:\n",
    "            identity  = int(identity)\n",
    "            attendance_df.loc[identity, 'Attendance'] = 'Present'\n",
    "            name = attendance_df.loc[identity, 'Name']\n",
    "            #print(f'Student ID : {identity}, Name : \\'{name}\\' Present')\n",
    "        \n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        frame = np.ascontiguousarray(frame, dtype=np.uint8)\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y + h),(0,255,0),3)\n",
    "        cv2.putText(frame, f'ID : {str(identity)}', (x,y-40), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 1)\n",
    "        cv2.putText(frame, f'Name : {str(name)}', (x,y-20), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 1)\n",
    "        \n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        #key = cv2.waitKey(1)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    now = now.strftime(\"%d-%b-%Y_%H-%M-%S\")\n",
    "    attendance_df.to_excel('Attendances/'+now+'_attendance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd27b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_excel('Students_information.xlsx').set_index('ID')\n",
    "inference(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e05ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_excel('Students_information.xlsx').set_index('ID')\n",
    "student_df.loc[990] = \"John Doe\"\n",
    "student_df.to_excel('Students_information.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85ed242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "students_df = pd.read_excel('Students_information.xlsx').set_index('ID')\n",
    "\n",
    "def new_entry_to_database(video_path, database_path = 'database', representations_path = 'representations'):\n",
    "    \n",
    "    #Confirmation: \n",
    "    while True:\n",
    "        identity = int(input('Enter Student ID : '))\n",
    "        name = input('Enter Student Name: ')\n",
    "        print(f'Student name {identity} . Press \\'y\\' to confirm')\n",
    "        key = input()\n",
    "        if key == 'y':\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    students_df.loc[identity] = name\n",
    "    \n",
    "    if video_path:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "    else: \n",
    "        cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    #embeddings = load_representations(representations_path)\n",
    "    \n",
    "    try:\n",
    "      while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        cv2.imshow(\"Playing\",frame)\n",
    "        #key = cv2.waitKey(1)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    save_path = database_path + f'/{str(identity)}/'\n",
    "    if os.path.isdir(save_path):\n",
    "        cv2.imwrite(save_path+f'ID_{str(identity)}_{name}.jpg', frame)\n",
    "    else:\n",
    "        os.mkdir(save_path)\n",
    "        cv2.imwrite(save_path+f'ID_{str(identity)}_{name}.jpg', frame)\n",
    "    \n",
    "    frame = frame[:, :, [2, 1, 0]]\n",
    "    target_frame_size = (IMG_SIZE,IMG_SIZE)\n",
    "    face_obj = extract_faces(frame, target_size = target_frame_size, enforce_detection = False)\n",
    "    emb  = np.squeeze(get_embedding(face_obj[0][0][0]))\n",
    "    \n",
    "    add_representation(identity, emb)\n",
    "    students_df.to_excel('Students_information.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5e07126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Student ID : 1002\n",
      "Enter Student Name: Anya Joya\n",
      "Student name 1002 . Press 'y' to confirm\n",
      "y\n",
      "Face embeddings saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "new_entry_to_database(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
